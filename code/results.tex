\documentclass[a4]{beamer}%
\usepackage{hyperref}%
\usepackage{times}%
\usepackage[today,draft]{svninfo}%
\graphicspath{{../results/20100322T151819/}}%
\title{Sparse Hebbian Learning}%
\subtitle{Efficient coding in natural scenes using sparse spike coding: A comparison of different homeostasis and sparsening schemes}
\author{Laurent Perrinet, \\%
INCM/CNRS 
e-mail: Laurent.Perrinet@incm.cnrs-mrs.fr\\
\url{http://www.incm.cnrs-mrs.fr/LaurentPerrinet/SparseHebbianLearning}
}%
\begin{document}
\svnInfo $Id: results.tex 4286 2010-11-09 20:11:04Z perrinet $
\begin{frame}
\titlepage %
\end{frame}

\begin{frame}
\frametitle{What is this document?}%

\begin{itemize}
  \item Show how one can learn in an unsupervised manner efficient codes for natural images.
  \item Show the results of quantitative methods for describing the efficiency using a set of scripts ang llustrate the results of the scripts over one session. It is generated automatically from the script.
  \item Supplementary material for the paper :
\end{itemize}

{\tiny
\bibliographystyle{plainnat}%
\begin{thebibliography}{3}
\bibitem[Perrinet(2010]{Perrinet10shl}
Laurent~U. Perrinet,
\newblock Role of homeostasis in learning sparse representations.
\newblock \emph{Neural Computation} (7) 22, July 2010.
\newblock URL \url{http://www.incm.cnrs-mrs.fr/LaurentPerrinet/Publications/Perrinet10shl}
\newblock  Doi:10.1162/neco.2010.05-08-795
\end{thebibliography} 
}

Last modification : \svnId \\
Scripts: \url{http://www.incm.cnrs-mrs.fr/LaurentPerrinet/SparseHebbianLearning}

\end{frame}

\begin{frame}
\frametitle{Parameters used in this session}%

\begin{itemize}
  \item The whole collection of simulation scripts is written with the intention of controlling and comparing the convergence of the algorithms and the relative effect of the different parameters. All scripts to reproduce the figures and supplementary material are available on the author's website (see \url{http://www.incm.cnrs-mrs.fr/LaurentPerrinet/SparseHebbianLearning}). 
    \item In particular, figures 2 to 4 were produced by the following scripts without any editing: respectively, \texttt{experiment\_learn.m}, \texttt{experiment\_code\_efficiency.m} and \texttt{experiment\_nonhomeo.m}.
     \item  Code revision \svnInfoRevision\  and experiment $\mathtt{20100322T151819}$ are used for this paper. The original parameters of {\sc SparseNet} were used for the CGF algorithm (see Table).%
\end{itemize}
\end{frame}

%
\begin{frame}
\frametitle{Parameters used in this session}%
{\tiny
\begin{table}%[htdp]
\caption{\textbf{Parameters used in the simulations and their corresponding notation in the paper.} Note that aSSC uses 5 parameters less than CGF. These parameters were determined from the original parameters from {\sc SparseNet} and then optimized using robustness analysis.}
\begin{center}
\begin{tabular}{|c|c|} \hline%
Description&Value\\\hline\hline %
%experiment & $20080417T175704$ \\ 
%script revision & mp-sparsenet-1.5 \\ 
%number of SVN's release & 603 \\ 
dimension of imagelets (CGF \& aSSC) & $L = 256$ \\ 
dimension of dictionary (CGF \& aSSC) & $M = 324$ \\ 
number of learning steps (CGF \& aSSC) & $40001$ \\ 
learning rate (CGF \& aSSC) & $\eta = 0.2$ \\ 
batch size (CGF \& aSSC) & $100$ \\ 
%noise variance (CGF) & $\sigma_n = 0.017$ \\ 
%noise variance (aSSC) & $\sigma_n = 0.008$ \\ 
%$eta-homeo$ & $0.01$ \\ 
learning rate of homeostasis (CGF) & $\eta_h = 0.025$ \\ 
learning rate of homeostasis  (aSSC) & $\eta_h = 0.01$ \\ 
%$n-quant$ & $256$ \\ 
homeostasis smoothing rate (CGF) & $\alpha = 0.02$ \\ 
desired variance (CGF) & $0.1$ \\ 
prior steepness (CGF) & $\beta =0.2$ \\ 
prior scaling (CGF) & $\sigma = 0.1$ \\ 
tolerance (CGF) & $tol = 0.0031$ \\ 
\hline\hline%
\end{tabular}
\end{center}
\label{tab:comp}
\end{table}%
}
\end{frame}

\begin{frame}
\frametitle{Parameters used in this session}%
\begin{center}
{\tiny
\begin{tabular}{l|l}
\input{../results/20100322T151819/default.txt}
\end{tabular}
}
\end{center}
\end{frame}
%%% ----------------------------------------------------------- %
\begin{frame}
\begin{figure}
\includegraphics[width=.33\textwidth]{fig_stats_imagelets.png}%
\includegraphics[width=.33\textwidth]{fig_stats_images_energy_hist}%
\includegraphics[width=.33\textwidth]{fig_stats_images_cov.png}%
\caption{ {\tiny\textbf{Control of the statistics of the inputs}. {\it (Left)} One set of imagelets drawn from SparseNet . Another set of images was used as a control and gave similar results. However, the images had to be fairly homogeneous (and therefore textured) since it happened to draw a patch from a flat area (such as the sky) in which case the signal was poor and the convergence of the learning was slower. When comparing the efficiency of two algorithms, we were careful to show for both the same set of imagelets. {\it (Middle)} Distribution of their energy per pixel  {\it (Right)} We show here a $16\times 16$ matrix of $16\times 16$ correlation values representing the covariance matrix of the set of images. This shows that luminance's correlation between 2 points is low (gray) compared to auto-correlation (white) when increasing the distance between both points to more than one pixel, validating the whitening hypothesis for the image's preprocessing.}}%
\label{fig:white}%}%
\end{figure}
\end{frame}
% ----------------------------------------------------------- %
\begin{frame}\frametitle{How do learning algorithms compare?}%
\begin{figure}
\only<1>{
\includegraphics[width=.49\textwidth]{fig_map_rand.png}%
\hspace{.019\textwidth}%
\includegraphics[width=.49\textwidth]{fig_map_rand.png}%
}%
\only<2>{
\includegraphics[width=.49\textwidth]{fig_map_cgf.png}%
\hspace{.019\textwidth}%
\includegraphics[width=.49\textwidth]{fig_map_ssc.png}%
}%

\caption{Set of RFs: \only<1>{initial dictionary}\only<2>{after SHL learning} (Left) SparseNet (Right) SSC. See \href{http://www.incm.cnrs-mrs.fr/LaurentPerrinet/Figures/Perrinet10shl/FigureDeux}{Fig. 2 in the paper}.  }%
\end{figure}%
\end{frame}

\begin{frame}\frametitle{Convergence of the learning}%experiment_learn.m$
\begin{figure}%
\includegraphics<1>[width=.5\textwidth]{fig_learn_energy}%
\includegraphics<1>[width=.5\textwidth]{fig_learn_ol}%
\includegraphics<2>[width=.5\textwidth]{fig_learn_ol_cost}%
\includegraphics<2>[width=.5\textwidth]{fig_learn_ol_phase}%
\includegraphics<3>[width=.5\textwidth]{fig_learn_L0}%
\includegraphics<3>[width=.5\textwidth]{fig_learn_kurtosis}%
\caption{Convergence of the learning: \only<1>{Energy and L1-Sparseness} \only<2>{Effect on the  L1-cost}   \only<3>{Effect on the  L0-cost + kurtosis} - evolution during the learning}%
\end{figure}%
\end{frame}%
%
%
\begin{frame}\frametitle{Who optimizes Olshausen's sparseness?}%
\begin{figure}%
	\includegraphics<1>[width=.5\textwidth]{fig_sparse_energy}%
	\includegraphics<1>[width=.5\textwidth]{fig_sparse_olshausen}%
	\includegraphics<2>[width=.5\textwidth]{fig_sparse_L0}%
	\includegraphics<2>[width=.5\textwidth]{fig_sparse_cost}%
%	\includegraphics<2>[width=.5\textwidth]{fig_sparse_phase}%
\caption{Tuning Olshausen's sparseness parameter: \only<1>{Energy / Sparseness} \only<2>{Effect on the cost  / in phase space}  }%
\end{figure}%
\end{frame}%
%
\begin{frame}\frametitle{Who optimizes sparseness?}%
\begin{figure}%
	\includegraphics<1>[width=5.25cm]{fig_efficiency_L1}  %
	\includegraphics<2>[width=5.25cm]{fig_efficiency_L0}  %
	\includegraphics<2>[width=5.25cm]{fig_efficiency_kurt}  %
\caption{\only<1>{Residual Energy versus  Olshausen's cost (similar to L1 norm)} \only<2>{Residual Energy  vs L0 norm / Kurtosis vs L0 norm} }%
\end{figure}%
\end{frame}%
\begin{frame}\frametitle{Who optimizes L0 sparseness?}%
\begin{figure}%
	\includegraphics<1>[width=5.25cm]{fig_efficiency_L0_omp}  %
	\includegraphics<1>[width=5.25cm]{fig_efficiency_L0_crossed}  %
\caption{Residual Energy versus L0 norm: Cross-validation with OMP (See \href{http://www.incm.cnrs-mrs.fr/LaurentPerrinet/Figures/Perrinet10shl/FigureTrois}{Fig. 3 in the paper})/ crossing dictionaries}%
\end{figure}%
\end{frame}%
\begin{frame}\frametitle{Who optimizes L0 sparseness?}%
\begin{figure}%
	\includegraphics[width=.5\textwidth]{fig_coeff}%
	\includegraphics[width=.5\textwidth]{fig_hist}%
\caption{Decrease of coefficients with the rank / Histogram of coefficients before and after learning for CGF (with parametrized prior) and SSC  (See \href{http://www.incm.cnrs-mrs.fr/LaurentPerrinet/Figures/Perrinet10shl/FigureTrois}{Fig. 3 in the paper}).}%
\end{figure}%
\end{frame}%

\begin{frame}\frametitle{Efficiency of homeostasis (SHL vs. AMP)}%
\begin{figure}%
	\includegraphics<1>[width=.49\textwidth]{fig_map_ssc.png}\hspace{.019\textwidth}%%
	\includegraphics<1>[width=.49\textwidth]{fig_map_ssc_nonhomeo.png}%
	\includegraphics<2>[width=.9\textwidth]{fig_nonhomeo_lut}%
	\includegraphics<3>[width=.33\textwidth]{fig_nonhomeo_L0}%
	\includegraphics<3>[width=.33\textwidth]{fig_nonhomeo_quant}% 
	\includegraphics<3>[width=.33\textwidth]{fig_nonhomeo_bits}% 
\caption{\only<1>{Map obtained with SSC vs. AMP  (See \href{http://www.incm.cnrs-mrs.fr/LaurentPerrinet/Figures/Perrinet10shl/FigureDeux}{Fig. 2} and  \href{http://www.incm.cnrs-mrs.fr/LaurentPerrinet/Figures/Perrinet10shl/FigureQuatre}{Fig. 4} in the paper) } \only<2>{Look-up Table for the different methods} \only<3>{Efficiency of AMP compared to SSC: (Left) L0 versus SE in simple coding (Middle) Quantization error using the LUT (Right) Final efficiency as the SE with quantization as a function of information (bits), that is, the L0 norm (See \href{http://www.incm.cnrs-mrs.fr/LaurentPerrinet/Figures/Perrinet10shl/FigureQuatre}{Fig. 4} in the paper).}}%
\end{figure}%
\end{frame}%



\begin{frame}\frametitle{Stability: perturbing SSC}%
\begin{figure}%
	\includegraphics<1>[width=.5\textwidth]{fig_map_perturb_init.png}%
 	\includegraphics<1>[width=.5\textwidth]{fig_perturb_init_P}%
 	\includegraphics<2>[width=.5\textwidth]{fig_map_perturb_final.png}%
 	\includegraphics<2>[width=.5\textwidth]{fig_perturb_final_P}%
\caption{\tiny As an adaptive algorithm, we checked that the system returned to a similar macroscopic state after a local perturbation (see upper-left corner in the map on the left).  \only<1>{To illustrate this, we perturb after convergence one filter (by re-initializing it to a random filter) and then resume the aSSC algorithm. The first effect is that the gain function of the perturbed neuron changes since the average value of the correlation coefficient drops for that neuron. As a consequence, the egalitarian homeostasis boosts the correlation value of this neuron relative to the other neurons so that the probability of choosing any neuron is still uniform.}\only<2>{  After a few learning steps, the filter retrieves an edge-like shape which is often close to the feature before to the perturbation, as this feature was momentarily absent from the representation dictionary. See script \texttt{experiment\_perturb.m} to reproduce this experiment.}}%
\end{figure}%
\end{frame}%

\begin{frame}\frametitle{Architecture: why not use symmetry?}%
\begin{figure}%
 	\includegraphics<1>[width=\textwidth]{fig_sym_ssc_map.png}  %
 	\includegraphics<2>[width=.5\textwidth]{fig_sym_efficiency_L0}%
 	\includegraphics<2>[width=.5\textwidth]{fig_sym_phase}%
\caption{\tiny \only<1>{ Another extension of the algorithm is to not use the implicit ON-OFF symmetry of filters which introduces the constraint that if a filter exists, then the symmetric filter exists, that is, that we rate the efficiency of a match by the absolute value of the correlation coefficient. The relaxed condition proved to be more efficient, suggesting that the symmetry that is observed is more a general effect and that since neurons are not linear only integrators with a rectifier, more efficient solutions may exist. To compare our algorithm with {\sc SparseNet}, we similarly assumed that in the dictionary, filters were symmetric. In fact, inspired by biology, receptive fields often coexist with opposite polarities (so-called ON/OFF symmetry). This is often assumed as a strict hypothesis in neural models and implies the constraint in the generative model that when looking for a match, the correlation could be positive or negative and therefore that the best match should be chosen as the greatest absolute value in the matching step of Matching Pursuit. }\only<2>{ If we instead choose a dictionary of twice the size and that we choose only the greatest values (that is not applying the absolute operator), we obtain a system where each spike would have the same informational cost (since $\log_2(2M)=\log_2(M)+1$, the additional bit replaces the polarity bit from the symmetric case). The dictionary obtained when releasing the symmetry constraint looks qualitatively similar but proves to be of slightly higher efficiency.} See script \texttt{experiment\_symmetric.m} to reproduce the figure.}%
\end{figure}%
\end{frame}%

\begin{frame}\frametitle{Architecture: trying different overcompleteness factors}%
\begin{figure}
 	\includegraphics<1>[width=.5\textwidth]{fig_stability_oc_ssc_1.png}  %
 	\includegraphics<2>[width=.5\textwidth]{fig_stability_oc_ssc_2.png}  %
 	\includegraphics<3>[width=.5\textwidth]{fig_stability_oc_ssc_3.png}  %
 	\includegraphics<4>[width=.5\textwidth]{fig_stability_oc_ssc_4.png}  %
 	\includegraphics<5>[width=.5\textwidth]{fig_stability_oc_ssc_5.png}  %
 	\includegraphics<6>[width=.49\textwidth]{fig_stability_oc_energy}%
 	\includegraphics<6>[width=.49\textwidth]{fig_stability_oc_L0}%
 	\includegraphics<7>[width=.49\textwidth]{fig_stability_oc_L0_res}%
 	\includegraphics<7>[width=.5\textwidth]{fig_stability_oc_occam}  %
\caption{\tiny \only<1-5>{We analyzed the effect of increasing the size of the dictionary, that is, by increasing the complexity of the representation, on the qualitative shape of receptive fields maps and on the global efficiency. When increasing over-completeness, one observes the emergence of different classes of filters: first different positions and edges, then edges with different orientations and phases, finally different frequencies and also textured or end-stopped cells. Exploring the efficiency results for different dimensions of the dictionary gave an evaluation of the optimal complexity of the LGM to describe imagelets in terms of a trade-off between accuracy and generality (not shown here). Changing the number of filters to }\only<1>{ $5^2$}\only<2>{ $8^2$}\only<3>{ $13^2$}\only<4>{ $21^2$}\only<5>{ $34^2$},  \only<6-7>{When increasing the number of coefficients, one sees qualitatively that the complexity of the features represented by the filters progressively increases. However, we see curved, textured or ``end-stopping'' cells only in the biggest map. This could be a result of a lack of convergence (we used the same number of steps for all sizes), but also the sign of a transition from representing edges and their different transforms. It seems that from the lower-dimension $5\times5$  map to the more complex $21\times21$, the dictionaries represent the invariance to different transforms, respectively position, scale and spatial frequency. An interesting perspective is to perform a cluster analysis on the dictionary to observe bifurcations as a function of complexity. See script \texttt{experiment\_stability\_oc.m} to reproduce the figure.}  }%
\end{figure}	
\end{frame}



\begin{frame}\frametitle{Controls: Robustness of the methods}%
%-----------------------------------------------------------------------------------------------------------------------------------%
\label{ann:robust}%

{\scriptsize
%
\begin{itemize}
  \item We include in our computational framework the ability to control the validity of our hypothesis but also of exploring the evolution of the efficiency of one model when changing one single parameter around the operating point that is chosen for the experience (see Table~\ref{tab:comp}). 
    \item We use perturbations of the parameters in order to show that they give locally the best efficiency but also that the comparison between algorithms is fair. This also allows to identify the parameters which are most relevant in the sense that small variations induces bigger changes of efficiency. This was in particular true for the {\sc SparseNet} algorithm: It is more sensitive to parameters (including learning rate, homeostasis parameter) than our solution. Tuning the parametric model and specifically parameters $\beta$ and $\sigma$, is of great importance. Note also that the learning rate depends on the dimension of the dictionary. In fact, in higher dimensions, the non-linear correlation coefficients are necessarily lower and thus the learning slower for each neuron. 
      \item See scripts \texttt{experiment\_stability\_eta.m}, \texttt{experiment\_stability\_homeo.m}, \texttt{experiment\_stability\_cgf.m}  to control {\sc SparseNet} and \texttt{experiment\_stability\_ssc.m}  to control aSSC. %
\end{itemize}

}

\end{frame}%


\begin{frame}\frametitle{Controls: different homeo in CGF and SSC}%
\begin{figure}%
\includegraphics<1>[width=.49\textwidth]{fig_stability_homeo_cgf_1.png}%
\includegraphics<2>[width=.49\textwidth]{fig_stability_homeo_cgf_2.png}%
\includegraphics<3>[width=.49\textwidth]{fig_stability_homeo_cgf_3.png}%
\includegraphics<4>[width=.49\textwidth]{fig_stability_homeo_cgf_4.png}%
\includegraphics<5>[width=.49\textwidth]{fig_stability_homeo_cgf_5.png}%
\includegraphics<6>[width=.49\textwidth]{fig_stability_homeo_cgf_6.png}%
\includegraphics<7>[width=.49\textwidth]{fig_stability_homeo_cgf_7.png}%
\includegraphics<8>[width=.49\textwidth]{fig_stability_homeo_cgf_8.png}%
\includegraphics<9>[width=.49\textwidth]{fig_stability_homeo_cgf_9.png}%
\hspace{.019\textwidth}%
\includegraphics<1>[width=.49\textwidth]{fig_stability_homeo_ssc_1.png}%
\includegraphics<2>[width=.49\textwidth]{fig_stability_homeo_ssc_2.png}%
\includegraphics<3>[width=.49\textwidth]{fig_stability_homeo_ssc_3.png}%
\includegraphics<4>[width=.49\textwidth]{fig_stability_homeo_ssc_4.png}%
\includegraphics<5>[width=.49\textwidth]{fig_stability_homeo_ssc_5.png}%
\includegraphics<6>[width=.49\textwidth]{fig_stability_homeo_ssc_6.png}%
\includegraphics<7>[width=.49\textwidth]{fig_stability_homeo_ssc_7.png}%
\includegraphics<8>[width=.49\textwidth]{fig_stability_homeo_ssc_8.png}%
\includegraphics<9>[width=.49\textwidth]{fig_stability_homeo_ssc_9.png}%
\includegraphics<10>[width=.33\textwidth]{fig_stability_homeo_energy} %
\includegraphics<10>[width=.33\textwidth]{fig_stability_homeo_sparseness}%
\includegraphics<10>[width=.33\textwidth]{fig_stability_homeo_cost}%	
\caption{Changing homeostasis parameter eta: \only<1-9>{maps at the end of a learning when multiplying rate in CGF/SSC by a factor}\only<1>{ of $0.$}\only<2>{ of  .17}\only<3>{ of  .31}\only<4>{ of .56}\only<5>{ of  1}\only<6>{ of 1.77}\only<7>{ of 3.1}\only<8>{ of 5.6}\only<9>{ of 10}  \only<10>{Summary of Energy / Olshausen's sparseness / cost with the variation of the parameters.} }%
\end{figure}%
\end{frame}%



\begin{frame}\frametitle{Controls: different learning rates $\eta$ CGF vs. SSC}%
\begin{figure}%
	\includegraphics<1>[width=.49\textwidth]{fig_stability_eta_cgf_1.png}%
	\includegraphics<2>[width=.49\textwidth]{fig_stability_eta_cgf_2.png}%
 	\includegraphics<3>[width=.49\textwidth]{fig_stability_eta_cgf_3.png}%
 	\includegraphics<4>[width=.49\textwidth]{fig_stability_eta_cgf_4.png}%
 	\includegraphics<5>[width=.49\textwidth]{fig_stability_eta_cgf_5.png}%
 	\includegraphics<6>[width=.49\textwidth]{fig_stability_eta_cgf_6.png}%
 	\includegraphics<7>[width=.49\textwidth]{fig_stability_eta_cgf_7.png}%
 	\includegraphics<8>[width=.49\textwidth]{fig_stability_eta_cgf_8.png}%
 	\includegraphics<9>[width=.49\textwidth]{fig_stability_eta_cgf_9.png}%
	\hspace{.019\textwidth}%
 	\includegraphics<1>[width=.49\textwidth]{fig_stability_eta_ssc_1.png}%
 	\includegraphics<2>[width=.49\textwidth]{fig_stability_eta_ssc_2.png}%
 	\includegraphics<3>[width=.49\textwidth]{fig_stability_eta_ssc_3.png}%
 	\includegraphics<4>[width=.49\textwidth]{fig_stability_eta_ssc_4.png}%
 	\includegraphics<5>[width=.49\textwidth]{fig_stability_eta_ssc_5.png}%
 	\includegraphics<6>[width=.49\textwidth]{fig_stability_eta_ssc_6.png}%
         \includegraphics<7>[width=.49\textwidth]{fig_stability_eta_ssc_7.png}%
 	\includegraphics<8>[width=.49\textwidth]{fig_stability_eta_ssc_8.png}%
 	\includegraphics<9>[width=.49\textwidth]{fig_stability_eta_ssc_9.png}%
	\includegraphics<10>[width=.33\textwidth]{fig_stability_eta_ol_cost}%
	\includegraphics<10>[width=.33\textwidth]{fig_stability_eta_energy}%
	\includegraphics<10>[width=.33\textwidth]{fig_stability_eta_sparseness}%
\caption{Optimizing learning parameter eta: \only<1-9>{multiplying in CGF/SSC by a factor}\only<1>{ of .1}\only<2>{ of  .177}\only<3>{ of  .316}\only<4>{ of .562}\only<5>{ of  1 (=no change)}\only<6>{ of 1.77}\only<7>{ of 3.16}\only<8>{ of 5.62}\only<9>{ of 10}\only<1-9>{, we optimized CGF and showed it's limits when changing parameters.  }\only<10>{Summary of Olshausen's cost / occam with the variation of the parameters.}  }%
\end{figure}
\end{frame}
 
 % experiment_stability_cgf
\begin{frame}\frametitle{Controls: different parameters for CGF: beta / sigma}%
\begin{figure}
 	\includegraphics<1>[width=.49\textwidth]{fig_stability_cgf_1_1.png}%
 	\includegraphics<2>[width=.49\textwidth]{fig_stability_cgf_1_2.png}%
 	\includegraphics<3>[width=.49\textwidth]{fig_stability_cgf_1_3.png}%
 	\includegraphics<4>[width=.49\textwidth]{fig_stability_cgf_1_4.png}%
 	\includegraphics<5>[width=.49\textwidth]{fig_stability_cgf_1_5.png}%
 	\includegraphics<6>[width=.49\textwidth]{fig_stability_cgf_1_6.png}%
 	\includegraphics<7>[width=.49\textwidth]{fig_stability_cgf_1_7.png}%
 	\includegraphics<8>[width=.49\textwidth]{fig_stability_cgf_1_8.png}%
 	\includegraphics<9>[width=.49\textwidth]{fig_stability_cgf_1_9.png}%	
	\hspace{.019\textwidth}%
 	\includegraphics<1>[width=.49\textwidth]{fig_stability_cgf_2_1.png}%
 	\includegraphics<2>[width=.49\textwidth]{fig_stability_cgf_2_2.png}%
 	\includegraphics<3>[width=.49\textwidth]{fig_stability_cgf_2_3.png}%
 	\includegraphics<4>[width=.49\textwidth]{fig_stability_cgf_2_4.png}%
 	\includegraphics<5>[width=.49\textwidth]{fig_stability_cgf_2_5.png}%
 	\includegraphics<6>[width=.49\textwidth]{fig_stability_cgf_2_6.png}%
 	\includegraphics<7>[width=.49\textwidth]{fig_stability_cgf_2_7.png}%
 	\includegraphics<8>[width=.49\textwidth]{fig_stability_cgf_2_8.png}%
 	\includegraphics<9>[width=.49\textwidth]{fig_stability_cgf_2_9.png}%	
 	\includegraphics<10>[width=.49\textwidth]{fig_stability_cgf_param_beta_sparseness_default}%
 	\includegraphics<10>[width=.49\textwidth]{fig_stability_cgf_param_sigma_sparseness_default}%
 	\includegraphics<11>[width=.49\textwidth]{fig_stability_cgf_param_beta_sparseness}%
 	\includegraphics<11>[width=.49\textwidth]{fig_stability_cgf_param_sigma_sparseness}%
 	\includegraphics<12>[width=.49\textwidth]{fig_stability_cgf_param_beta_res}%
 	\includegraphics<12>[width=.49\textwidth]{fig_stability_cgf_param_sigma_res}%
 	\includegraphics<13>[width=.49\textwidth]{fig_stability_cgf_param_beta}%
 	\includegraphics<13>[width=.49\textwidth]{fig_stability_cgf_param_sigma}%
\caption{Optimizing CGF parameters:  \only<1-9>{multiplying respectively alpha and beta by a factor}\only<1>{ of .1}\only<2>{ of  .177}\only<3>{ of  .316}\only<4>{ of .562}\only<5>{ of  1 (=no change)}\only<6>{ of 1.77}\only<7>{ of 3.16}\only<8>{ of 5.62}\only<9>{ of 10}\only<1-9>{, we optimized CGF and showed it's limits when changing parameters. } \only<10>{Summary of Olshausen's cost with the variation of the parameters:}\only<10>{ (Left) alpha / (Right) beta}  }%\only<1>{ of .01}\only<2>{ of  .0316}\only<3>{ of  .1}\only<4>{ of .316}\only<5>{ of  1 (=no change)}\only<6>{ of 3.16}\only<7>{ of 10}\only<8>{ of 31.6}\only<9>{ of 100}
\end{figure}
\end{frame}
\begin{frame}\frametitle{Controls: different parameters for CGF: tol / alpha}%
\begin{figure}
 	\includegraphics<1>[width=.49\textwidth]{fig_stability_cgf_3_1.png}%
 	\includegraphics<2>[width=.49\textwidth]{fig_stability_cgf_3_2.png}%
 	\includegraphics<3>[width=.49\textwidth]{fig_stability_cgf_3_3.png}%
 	\includegraphics<4>[width=.49\textwidth]{fig_stability_cgf_3_4.png}%
 	\includegraphics<5>[width=.49\textwidth]{fig_stability_cgf_3_5.png}%
 	\includegraphics<6>[width=.49\textwidth]{fig_stability_cgf_3_6.png}%
 	\includegraphics<7>[width=.49\textwidth]{fig_stability_cgf_3_7.png}%
 	\includegraphics<8>[width=.49\textwidth]{fig_stability_cgf_3_8.png}%
 	\includegraphics<9>[width=.49\textwidth]{fig_stability_cgf_3_9.png}%	
	\hspace{.019\textwidth}%
 	\includegraphics<1>[width=.49\textwidth]{fig_stability_cgf_4_1.png}%
 	\includegraphics<2>[width=.49\textwidth]{fig_stability_cgf_4_2.png}%
 	\includegraphics<3>[width=.49\textwidth]{fig_stability_cgf_4_3.png}%
 	\includegraphics<4>[width=.49\textwidth]{fig_stability_cgf_4_4.png}%
 	\includegraphics<5>[width=.49\textwidth]{fig_stability_cgf_4_5.png}%
 	\includegraphics<6>[width=.49\textwidth]{fig_stability_cgf_4_6.png}%
 	\includegraphics<7>[width=.49\textwidth]{fig_stability_cgf_4_7.png}%
 	\includegraphics<8>[width=.49\textwidth]{fig_stability_cgf_4_8.png}%
 	\includegraphics<9>[width=.49\textwidth]{fig_stability_cgf_4_9.png}%
 	\includegraphics<10>[width=.5\textwidth]{fig_stability_cgf_param_tol_sparseness_default}%
 	\includegraphics<10>[width=.49\textwidth]{fig_stability_cgf_param_alpha_sparseness_default}%
\caption{Optimizing CGF parameters:  \only<1-9>{multiplying respectively sigma and tol by a factor}\only<1>{ of .1}\only<2>{ of  .177}\only<3>{ of  . .316}\only<4>{ of .562}\only<5>{ of  1 (=no change)}\only<6>{ of 1.77}\only<7>{ of 3.16}\only<8>{ of 5.62}\only<9>{ of 10}\only<1-9>{, we optimized CGF and showed it's limits when changing parameters. } \only<10>{Summary of Olshausen's cost with the variation of the parameters:}\only<10>{ (Left) tol / (Right) alpha} }%\only<1>{ of .01}\only<2>{ of  .0316}\only<3>{ of  .1}\only<4>{ of .316}\only<5>{ of  1 (=no change)}\only<6>{ of 3.16}\only<7>{ of 10}\only<8>{ of 31.6}\only<9>{ of 100}
\end{figure}
\end{frame}
 % 
%
\begin{frame}\frametitle{Controls: different parameters for SSC: frac / noise}%
\begin{figure}
 	\includegraphics<1>[width=.49\textwidth]{fig_stability_ssc_frac_1.png}%
 	\includegraphics<2>[width=.49\textwidth]{fig_stability_ssc_frac_2.png}%
 	\includegraphics<3>[width=.49\textwidth]{fig_stability_ssc_frac_3.png}%
 	\includegraphics<4>[width=.49\textwidth]{fig_stability_ssc_frac_4.png}%
 	\includegraphics<5>[width=.49\textwidth]{fig_stability_ssc_frac_5.png}%
 	\includegraphics<6>[width=.49\textwidth]{fig_stability_ssc_frac_6.png}%
 	\includegraphics<7>[width=.49\textwidth]{fig_stability_ssc_frac_7.png}%
 	\includegraphics<8>[width=.49\textwidth]{fig_stability_ssc_frac_8.png}%
 	\includegraphics<9>[width=.49\textwidth]{fig_stability_ssc_frac_9.png}%	
	\hspace{.019\textwidth}%
 	\includegraphics<1>[width=.49\textwidth]{fig_stability_ssc_noise_1.png}%
 	\includegraphics<2>[width=.49\textwidth]{fig_stability_ssc_noise_2.png}%
 	\includegraphics<3>[width=.49\textwidth]{fig_stability_ssc_noise_3.png}%
 	\includegraphics<4>[width=.49\textwidth]{fig_stability_ssc_noise_4.png}%
 	\includegraphics<5>[width=.49\textwidth]{fig_stability_ssc_noise_5.png}%
 	\includegraphics<6>[width=.49\textwidth]{fig_stability_ssc_noise_6.png}%
 	\includegraphics<7>[width=.49\textwidth]{fig_stability_ssc_noise_7.png}%
 	\includegraphics<8>[width=.49\textwidth]{fig_stability_ssc_noise_8.png}%
 	\includegraphics<9>[width=.49\textwidth]{fig_stability_ssc_noise_9.png}%	
 	\includegraphics<10>[width=.49\textwidth]{fig_stability_ssc_frac_sparseness}%
 	\includegraphics<10>[width=.49\textwidth]{fig_stability_ssc_noise_sparseness}%
\caption{Optimizing SSC parameters:  \only<1-9>{using different values for frac and noise. } \only<10>{summary of Olshausen's cost with the variation of the parameters:}\only<10>{ (Left) frac / (Right) noise}  }%
\end{figure}
\end{frame}

 
\begin{frame}\frametitle{Controls: using different image databases}%
\begin{figure}%
	\includegraphics<1>[width=.49\textwidth]{fig_stability_imagebase_icabench.png}\hspace{.019\textwidth}%%
 	\includegraphics<1>[width=.49\textwidth]{fig_stability_imagebase_yelmo.png}%
\caption{\tiny We tested the algorithm on 2 different databases from  the one used in SparseNet. (Left) ICAbanch database (Right) Personal database of holiday pictures.}%
\end{figure}%
\end{frame}%
 
\end{document}
